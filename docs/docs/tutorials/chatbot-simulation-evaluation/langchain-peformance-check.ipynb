{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96cddf-5fb6-4208-b6c4-8d35efe341e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  langchain langsmith langchainhub --quiet\n",
    "!pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-search langchain_community --quiet\n",
    "!pip install -U langgraph langchain langsmith langchain_openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b255-7dd3-4991-baf8-daad6149f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "# with tracing_v2_enabled(project_name=\"My Project\"):\n",
    "#     agent.run(\"How many people live in canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c64d13-5eeb-4f7e-ab9f-be47c65ba796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Performance Check - {unique_id}\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05325b9f-879d-4147-819e-440dc59a2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f81127-6a3c-440b-9fd0-972271e74d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Fetches the latest version of this prompt\n",
    "prompt = hub.pull(\"wfh/langsmith-agent-prompt:5d466cbc\")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "#     temperature=0,\n",
    "# )\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8989\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "# API_KEY= \"alanliuxiang\"\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"http://localhost:8000\"\n",
    "# MODEL_NAME = \"ibm-granite/granite-3.1-8b-base\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [\n",
    "    DuckDuckGoSearchResults(\n",
    "        name=\"duck_duck_go\"\n",
    "    ),  # General internet search using DuckDuckGo\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "runnable_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=runnable_agent,\n",
    "    tools=tools,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ae847-c6ab-4d61-8843-c739b0737eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What's LangSmith?\",\n",
    "    \"When was Llama-v2 released?\",\n",
    "    \"What is the langsmith cookbook?\",\n",
    "    \"When did langchain first announce the hub?\",\n",
    "]\n",
    "\n",
    "results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356cec6b-f40a-4d24-8f45-010cfcdd6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3d51b-a18b-4d58-aa7e-68d4944d96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",\n",
    "    \"July 18, 2023\",\n",
    "    \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\",\n",
    "    \"September 5, 2023\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daae4d1-c6d0-452a-809f-78049fd5a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"agent-qa-{unique_id}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name,\n",
    "    description=\"An example dataset of questions over the LangSmith documentation.\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"input\": query} for query in inputs],\n",
    "    outputs=[{\"output\": answer} for answer in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed2cbf-6c35-4770-b5b2-abf08668df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def create_agent(prompt, llm_with_tools):\n",
    "    runnable_agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_with_tools\n",
    "        | OpenAIToolsAgentOutputParser()\n",
    "    )\n",
    "    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa1ccd-6104-4fd8-8584-7b3ebc10c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def check_not_idk(run: Run, example: Example):\n",
    "    \"\"\"Illustration of a custom evaluator.\"\"\"\n",
    "    agent_response = run.outputs[\"output\"]\n",
    "    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    # You can access the dataset labels in example.outputs[key]\n",
    "    # You can also access the model inputs in run.inputs[key]\n",
    "    return EvaluationResult(\n",
    "        key=\"not_uncertain\",\n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa148b-e6c5-4906-9195-ae54820805ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def max_pred_length(runs: List[Run], examples: List[Example]):\n",
    "    predictions = [len(run.outputs[\"output\"]) for run in runs]\n",
    "    return EvaluationResult(key=\"max_pred_length\", score=max(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99260b12-8f34-420b-b94e-646f0cd52a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        check_not_idk,\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "        # Measure the embedding distance between the output and the reference answer\n",
    "        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n",
    "        EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        # Grade whether the output satisfies the stated criteria.\n",
    "        # You can select a default one such as \"helpfulness\" or provide your own.\n",
    "        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n",
    "        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n",
    "        # You can use default criteria or write our own rubric\n",
    "        RunEvalConfig.LabeledScoreString(\n",
    "            {\n",
    "                \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "            },\n",
    "            normalize_by=10,\n",
    "        ),\n",
    "    ],\n",
    "    batch_evaluators=[max_pred_length],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03a850-2979-4aaf-b025-89a9357e333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# We will test this version of the prompt\n",
    "prompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d6922-2482-4fab-b7f3-5aa748454a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.smith import arun_on_dataset, run_on_dataset\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "# Chains may have memory. Passing in a constructor function lets the\n",
    "# evaluation framework avoid cross-contamination between runs.\n",
    "def construct_chain():\n",
    "    # llm = ChatOpenAI(temperature=0)\n",
    "    chain = LLMChain.from_string(\n",
    "        llm,\n",
    "        \"What's the answer to {your_input_key}\"\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",  # \"Correctness\" against a reference answer\n",
    "        \"embedding_distance\",\n",
    "        RunEvalConfig.Criteria(\"helpfulness\"),\n",
    "        RunEvalConfig.Criteria({\n",
    "            \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\n",
    "        }),\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client()\n",
    "run_on_dataset(\n",
    "    client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=construct_chain,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0a78e-7718-46c0-8261-9fcc5766758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from langchain.smith import arun_on_dataset, run_on_dataset\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=functools.partial(\n",
    "        create_agent, prompt=prompt,\n",
    "        llm_with_tools=llm_with_tools\n",
    "    ),\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",\n",
    "    # Project metadata communicates the experiment parameters,\n",
    "    # Useful for reviewing the test results\n",
    "    # project_metadata={\n",
    "    #     \"env\": \"testing-notebook\",\n",
    "    #     \"model\": llm,\n",
    "    #     \"prompt\": \"5d466cbc\",\n",
    "    # },\n",
    ")\n",
    "\n",
    "# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n",
    "# These are logged as warnings here and captured as errors in the tracing UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88814476-5945-4368-89ae-419553552307",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_results.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
